{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "## Solving the linear regression problem with gradient descent\n",
    "\n",
    "Today we rewise the linear regression algorithm and it's gradient solution.\n",
    "\n",
    "Your main goal will be to __derive and implement the gradient of MSE, MAE, L1 and L2 regularization terms__ respectively in general __vector form__ (when both single observation $\\mathbf{x}_i$ and corresponding target value $\\mathbf{y}_i$ are vectors).\n",
    "\n",
    "This techniques will be useful later in Deep Learning module of out course as well.\n",
    "\n",
    "We will work with [Boston housing prices dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html), subset, which was preprocessed for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Run some setup code for this notebook.\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('boston_subset.json', 'r') as iofile:\n",
    "    dataset = json.load(iofile)\n",
    "feature_matrix = np.array(dataset['data'])\n",
    "targets = np.array(dataset['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoreload is a great stuff, but sometimes it does not work as intended. The code below aims to fix than. __Do not forget to save your changes in the `.py` file before reloading the desired functions.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warming up: matrix differentiation\n",
    "_You will meet this questions later in Labs as well, so we highly recommend to answer to them right here._\n",
    "\n",
    "Credits: this theretical part is copied from [YSDA Practical_DL course](https://github.com/yandexdataschool/Practical_DL/tree/spring2019/homework01) homework01."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it easy to google every task please please please try to undestand what's going on. The \"just answer\" thing will be not counted, make sure to present derivation of your solution. It is absolutely OK if you found an answer on web then just exercise in $\\LaTeX$ copying it into here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links: \n",
    "[1](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)\n",
    "[2](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)\n",
    "[3](http://cal.cs.illinois.edu/~johannes/research/matrix%20calculus.pdf)\n",
    "[4](http://research.microsoft.com/en-us/um/people/cmbishop/prml/index.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline question 1\n",
    "$$  \n",
    "y = x^Tx,  \\quad x \\in \\mathbb{R}^N \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline question 2\n",
    "$$ y = tr(AB) \\quad A,B \\in \\mathbb{R}^{N \\times N} $$ \n",
    "\n",
    "$$\n",
    "\\frac{dy}{dA} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline question 3\n",
    "$$  \n",
    "y = x^TAc , \\quad A\\in \\mathbb{R}^{N \\times N}, x\\in \\mathbb{R}^{N}, c\\in \\mathbb{R}^{N} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} =\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dA} =\n",
    "$$\n",
    "\n",
    "Hint for the latter (one of the ways): use *ex. 2* result and the fact \n",
    "$$\n",
    "tr(ABC) = tr (CAB)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions and derivetives implementation\n",
    "You will need to implement the methods from `loss_and_derivatives.py` to go further.\n",
    "__In this assignment we ignore the bias term__, so the linear model takes simple form of \n",
    "$$\n",
    "\\hat{\\mathbf{y}} = XW\n",
    "$$\n",
    "\n",
    "Implement the loss functions, regularization terms and their derivatives w.r.t weighs matrix. \n",
    "\n",
    "_Once again, you can assume that linar model does not need bias term for now. The dataset is preprocessed for this case._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# This dirty hack might help if the autoreload has failed for some reason\n",
    "try:\n",
    "    del LossAndDerivatives\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from loss_and_derivatives import LossAndDerivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1., 1.])\n",
    "x_n, y_n = feature_matrix, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here come several asserts to check yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1., 1.])\n",
    "x_n, y_n = feature_matrix, targets\n",
    "\n",
    "w = np.vstack([w[None, :] + 0.27, w[None, :] + 0.22, w[None, :] + 0.45, w[None, :] + 0.1]).T\n",
    "y_n = np.hstack([y_n[:, None], 2*y_n[:, None], 3*y_n[:, None], 4*y_n[:, None]])\n",
    "\n",
    "x_t = torch.tensor(x_n)\n",
    "y_t = torch.tensor(y_n)\n",
    "w_t = torch.tensor(w, requires_grad=True) #torch.ones((2), dtype=torch.float64, requires_grad=True) + 0.27\n",
    "\n",
    "assert np.array_equal(w_t.detach().numpy(), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE + L2\n",
    "reg_coeff = 0.05\n",
    "loss1 = torch.mean((torch.matmul(x_t, w_t) - y_t)**2)\n",
    "loss1.backward()\n",
    "assert(np.allclose(w_t.grad.data.numpy(), LossAndDerivatives.mse_derivative(x_n, y_n, w), rtol=1e-3))\n",
    "\n",
    "print(w_t.grad.data.numpy())\n",
    "w_t.grad.data.zero_()\n",
    "print()\n",
    "\n",
    "loss2 = reg_coeff * torch.sum(w_t**2)\n",
    "loss2.backward()\n",
    "print(w_t.grad.data.numpy())\n",
    "w_t.grad.data.zero_()\n",
    "print((loss1 + loss2).item())\n",
    "print()\n",
    "print()\n",
    "\n",
    "loss_n = np.mean((x_n.dot(w) - y_n)**2) + reg_coeff * np.sum(w**2)\n",
    "print(loss_n, '\\n', LossAndDerivatives.mse_derivative(x_n, y_n, w), '\\n\\n', reg_coeff * LossAndDerivatives.l2_reg_derivative(w), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE + L1\n",
    "reg_coeff = 0.05\n",
    "loss1 = torch.mean((torch.matmul(x_t, w_t) - y_t)**2)\n",
    "loss1.backward()\n",
    "print(w_t.grad.data.numpy())\n",
    "# assert(np.allclose(w_t.grad.data.numpy(), LossAndDerivatives.mse_derivative(x_n, y_n, w), rtol=1e-3))\n",
    "\n",
    "w_t.grad.data.zero_()\n",
    "print()\n",
    "print()\n",
    "\n",
    "loss2 = reg_coeff * torch.sum(torch.abs(w_t))\n",
    "loss2.backward()\n",
    "print(w_t.grad.data.numpy())\n",
    "# assert(np.allclose(w_t.grad.data.numpy(), LossAndDerivatives.l1_reg_derivative(w), rtol=1e-3))\n",
    "\n",
    "w_t.grad.data.zero_()\n",
    "print((loss1 + loss2).item())\n",
    "print()\n",
    "print()\n",
    "\n",
    "loss_n = np.mean(np.abs((x_n.dot(w) - y_n))) + reg_coeff * np.sum(np.abs(w))\n",
    "print(loss_n, '\\n', LossAndDerivatives.mse_derivative(x_n, y_n, w), '\\n\\n', reg_coeff * LossAndDerivatives.l1_reg_derivative(w), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE + L2\n",
    "reg_coeff = 0.05\n",
    "loss1 = torch.mean(torch.abs(torch.matmul(x_t, w_t) - y_t))\n",
    "loss1.backward()\n",
    "assert(np.allclose(w_t.grad.data.numpy(), LossAndDerivatives.mae_derivative(x_n, y_n, w), rtol=1e-3))\n",
    "\n",
    "print(w_t.grad.data.numpy())\n",
    "w_t.grad.data.zero_()\n",
    "print()\n",
    "\n",
    "loss2 = reg_coeff * torch.sum(w_t**2)\n",
    "loss2.backward()\n",
    "print(w_t.grad.data.numpy())\n",
    "w_t.grad.data.zero_()\n",
    "print((loss1 + loss2).item())\n",
    "print()\n",
    "print()\n",
    "\n",
    "loss_n = np.mean(np.abs((x_n.dot(w) - y_n))) + reg_coeff * np.sum(w**2)\n",
    "print(loss_n, '\\n', LossAndDerivatives.mae_derivative(x_n, y_n, w), '\\n\\n', reg_coeff * LossAndDerivatives.l2_reg_derivative(w), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE + L1\n",
    "reg_coeff = 0.05\n",
    "loss1 = torch.mean(torch.abs(torch.matmul(x_t, w_t) - y_t))\n",
    "loss1.backward()\n",
    "assert(np.allclose(w_t.grad.data.numpy(), LossAndDerivatives.mae_derivative(x_n, y_n, w), rtol=1e-3))\n",
    "\n",
    "print(w_t.grad.data.numpy())\n",
    "w_t.grad.data.zero_()\n",
    "print()\n",
    "\n",
    "loss2 = reg_coeff * torch.sum(torch.abs(w_t))\n",
    "loss2.backward()\n",
    "print(w_t.grad.data.numpy())\n",
    "w_t.grad.data.zero_()\n",
    "print((loss1 + loss2).item())\n",
    "print()\n",
    "print()\n",
    "\n",
    "loss_n = np.mean(np.abs((x_n.dot(w) - y_n))) + reg_coeff * np.sum(np.abs(w))\n",
    "print(loss_n, '\\n', LossAndDerivatives.mae_derivative(x_n, y_n, w), '\\n\\n', reg_coeff * LossAndDerivatives.l1_reg_derivative(w), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes small loop with gradient descent algorithm. We compute the gradient over the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_by_grad(X, Y, w_0, loss_mode='mse', reg_mode=None, lr=0.05, n_steps=100, reg_coeff=0.05):\n",
    "    if loss_mode == 'mse':\n",
    "        loss_function = LossAndDerivatives.mse\n",
    "        loss_derivative = LossAndDerivatives.mse_derivative\n",
    "    elif loss_mode == 'mae':\n",
    "        loss_function = LossAndDerivatives.mae\n",
    "        loss_derivative = LossAndDerivatives.mae_derivative\n",
    "    else:\n",
    "        raise ValueError('Unknown loss function. Available loss functions: `mse`, `mae`')\n",
    "    \n",
    "    if reg_mode is None:\n",
    "        reg_function = LossAndDerivatives.no_reg\n",
    "        reg_derivative = LossAndDerivatives.no_reg_derivative # lambda w: np.zeros_like(w)\n",
    "    elif reg_mode == 'l2':\n",
    "        reg_function = LossAndDerivatives.l2_reg\n",
    "        reg_derivative = LossAndDerivatives.l2_reg_derivative\n",
    "    elif reg_mode == 'l1':\n",
    "        reg_function = LossAndDerivatives.l1_reg\n",
    "        reg_derivative = LossAndDerivatives.l1_reg_derivative\n",
    "    else:\n",
    "        raise ValueError('Unknown regularization mode. Avaliable modes: `l1`, `l2`, None')\n",
    "    \n",
    "    \n",
    "    w = w_0.copy()\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        empirical_risk = loss_function(X, Y, w) + reg_coeff * reg_function(w)\n",
    "        gradient = loss_derivative(X, Y, w) + reg_coeff * reg_derivative(w)\n",
    "        gradient_norm = np.linalg.norm(gradient)\n",
    "        if gradient_norm > 5.:\n",
    "            gradient = gradient / gradient_norm * 5.\n",
    "        w -= lr * gradient\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            print('Step={}, loss={}, gradient values={}\\n'.format(i, empirical_risk, gradient))\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial weights matrix\n",
    "w = np.ones((2,2), dtype=float)\n",
    "y_n = np.hstack([targets[:, None], targets[:, None]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "w_grad = get_w_by_grad(x_n, y_n, w, lr=0.05, loss_mode='mse', reg_mode='l1', reg_coeff=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x_n[:, -1], y_n[:, -1])\n",
    "plt.scatter(x_n[:, -1], x_n.dot(w_grad)[:, -1], color='orange', linewidth=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting your work\n",
    "To submit your work you need to log into Yandex contest (link will be provided later) and upload the `loss_and_derivatives.py` file for the corresponding problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 research env",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
