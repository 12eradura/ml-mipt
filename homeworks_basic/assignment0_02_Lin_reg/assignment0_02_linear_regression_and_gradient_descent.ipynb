{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "pdf-title"
    ]
   },
   "source": [
    "## Solving the linear regression problem with gradient descent\n",
    "\n",
    "Today we rewise the linear regression algorithm and it's gradient solution.\n",
    "\n",
    "Your main goal will be to __derive and implement the gradient of MSE, MAE, L1 and L2 regularization terms__ respectively in general __vector form__ (when both single observation $\\mathbf{x}_i$ and corresponding target value $\\mathbf{y}_i$ are vectors).\n",
    "\n",
    "This techniques will be useful later in Deep Learning module of out course as well.\n",
    "\n",
    "We will work with [Boston housing prices dataset](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_boston.html), subset, which was preprocessed for your convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# Run some setup code for this notebook.\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Some more magic so that the notebook will reload external python modules;\n",
    "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('boston_subset.json', 'r') as iofile:\n",
    "    dataset = json.load(iofile)\n",
    "feature_matrix = np.array(dataset['data'])\n",
    "targets = np.array(dataset['target'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Autoreload is a great stuff, but sometimes it does not work as intended. The code below aims to fix than. __Do not forget to save your changes in the `.py` file before reloading the desired functions.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Warming up: matrix differentiation\n",
    "_You will meet this questions later in Labs as well, so we highly recommend to answer to them right here._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since it easy to google every task please please please try to undestand what's going on. The \"just answer\" thing will be not counted, make sure to present derivation of your solution. It is absolutely OK if you found an answer on web then just exercise in $\\LaTeX$ copying it into here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Useful links: \n",
    "[1](http://www.machinelearning.ru/wiki/images/2/2a/Matrix-Gauss.pdf)\n",
    "[2](http://www.atmos.washington.edu/~dennis/MatrixCalculus.pdf)\n",
    "[3](http://cal.cs.illinois.edu/~johannes/research/matrix%20calculus.pdf)\n",
    "[4](http://research.microsoft.com/en-us/um/people/cmbishop/prml/index.htm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline question 1\n",
    "$$  \n",
    "y = x^Tx,  \\quad x \\in \\mathbb{R}^N \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} = \n",
    "$$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline question 2\n",
    "$$ y = tr(AB) \\quad A,B \\in \\mathbb{R}^{N \\times N} $$ \n",
    "\n",
    "$$\n",
    "\\frac{dy}{dA} =\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Inline question 3\n",
    "$$  \n",
    "y = x^TAc , \\quad A\\in \\mathbb{R}^{N \\times N}, x\\in \\mathbb{R}^{N}, c\\in \\mathbb{R}^{N} \n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dx} =\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dA} =\n",
    "$$\n",
    "\n",
    "Hint for the latter (one of the ways): use *ex. 2* result and the fact \n",
    "$$\n",
    "tr(ABC) = tr (CAB)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loss functions and derivetives implementation\n",
    "You will need to implement the methods from `loss_and_derivatives.py` to go further.\n",
    "__In this assignment we ignore the bias term__, so the linear model takes simple form of \n",
    "$$\n",
    "\\hat{\\mathbf{y}} = XW\n",
    "$$\n",
    "\n",
    "Implement the loss functions, regularization terms and their derivatives w.r.t weighs matrix. \n",
    "\n",
    "_Once again, you can assume that linar model does not need bias term for now. The dataset is preprocessed for this case._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": [
     "pdf-ignore"
    ]
   },
   "outputs": [],
   "source": [
    "# This dirty hack might help if the autoreload has failed for some reason\n",
    "try:\n",
    "    del LossAndDerivatives\n",
    "except:\n",
    "    pass\n",
    "\n",
    "from loss_and_derivatives import LossAndDerivatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1., 1.])\n",
    "x_n, y_n = feature_matrix, targets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here come several asserts to check yourself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "w = np.array([1., 1.])\n",
    "x_n, y_n = feature_matrix, targets\n",
    "\n",
    "w = np.vstack([w[None, :] + 0.27, w[None, :] + 0.22, w[None, :] + 0.45, w[None, :] + 0.1]).T\n",
    "y_n = np.hstack([y_n[:, None], 2*y_n[:, None], 3*y_n[:, None], 4*y_n[:, None]])\n",
    "\n",
    "x_t = torch.tensor(x_n)\n",
    "y_t = torch.tensor(y_n)\n",
    "w_t = torch.tensor(w, requires_grad=True) #torch.ones((2), dtype=torch.float64, requires_grad=True) + 0.27\n",
    "\n",
    "assert np.array_equal(w_t.detach().numpy(), w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.32890068 12.88731311 18.82128365 23.97731238]\n",
      " [ 9.55674399 17.05397661 24.98807528 32.01723714]]\n",
      "\n",
      "[[0.127 0.122 0.145 0.11 ]\n",
      " [0.127 0.122 0.145 0.11 ]]\n",
      "1468.7594312537688\n",
      "\n",
      "\n",
      "1468.7594312537685 \n",
      " [[ 7.32890068 12.88731311 18.82128365 23.97731238]\n",
      " [ 9.55674399 17.05397661 24.98807528 32.01723714]] \n",
      "\n",
      " [[0.127 0.122 0.145 0.11 ]\n",
      " [0.127 0.122 0.145 0.11 ]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MSE + L2\n",
    "reg_coeff = 0.05\n",
    "loss1 = torch.mean((torch.matmul(x_t, w_t) - y_t)**2)\n",
    "loss1.backward()\n",
    "assert(np.allclose(w_t.grad.data.numpy(), LossAndDerivatives.mse_derivative(x_n, y_n, w), rtol=1e-3))\n",
    "\n",
    "print(w_t.grad.data.numpy())\n",
    "w_t.grad.data.zero_()\n",
    "print()\n",
    "\n",
    "loss2 = reg_coeff * torch.sum(w_t**2)\n",
    "loss2.backward()\n",
    "print(w_t.grad.data.numpy())\n",
    "w_t.grad.data.zero_()\n",
    "print((loss1 + loss2).item())\n",
    "print()\n",
    "print()\n",
    "\n",
    "loss_n = np.mean((x_n.dot(w) - y_n)**2) + reg_coeff * np.sum(w**2)\n",
    "print(loss_n, '\\n', LossAndDerivatives.mse_derivative(x_n, y_n, w), '\\n\\n', reg_coeff * LossAndDerivatives.l2_reg_derivative(w), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 7.32890068 12.88731311 18.82128365 23.97731238]\n",
      " [ 9.55674399 17.05397661 24.98807528 32.01723714]]\n",
      "\n",
      "\n",
      "[[0.05 0.05 0.05 0.05]\n",
      " [0.05 0.05 0.05 0.05]]\n",
      "1468.6220512537686\n",
      "\n",
      "\n",
      "32.12463606879173 \n",
      " [[ 7.32890068 12.88731311 18.82128365 23.97731238]\n",
      " [ 9.55674399 17.05397661 24.98807528 32.01723714]] \n",
      "\n",
      " [[0.05 0.05 0.05 0.05]\n",
      " [0.05 0.05 0.05 0.05]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MSE + L1\n",
    "reg_coeff = 0.05\n",
    "loss1 = torch.mean((torch.matmul(x_t, w_t) - y_t)**2)\n",
    "loss1.backward()\n",
    "print(w_t.grad.data.numpy())\n",
    "# assert(np.allclose(w_t.grad.data.numpy(), LossAndDerivatives.mse_derivative(x_n, y_n, w), rtol=1e-3))\n",
    "\n",
    "w_t.grad.data.zero_()\n",
    "print()\n",
    "print()\n",
    "\n",
    "loss2 = reg_coeff * torch.sum(torch.abs(w_t))\n",
    "loss2.backward()\n",
    "print(w_t.grad.data.numpy())\n",
    "# assert(np.allclose(w_t.grad.data.numpy(), LossAndDerivatives.l1_reg_derivative(w), rtol=1e-3))\n",
    "\n",
    "w_t.grad.data.zero_()\n",
    "print((loss1 + loss2).item())\n",
    "print()\n",
    "print()\n",
    "\n",
    "loss_n = np.mean(np.abs((x_n.dot(w) - y_n))) + reg_coeff * np.sum(np.abs(w))\n",
    "print(loss_n, '\\n', LossAndDerivatives.mse_derivative(x_n, y_n, w), '\\n\\n', reg_coeff * LossAndDerivatives.l1_reg_derivative(w), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19708867 0.19621798 0.19621798 0.19572906]\n",
      " [0.25574138 0.25524507 0.25524507 0.25406404]]\n",
      "\n",
      "[[0.127 0.122 0.145 0.11 ]\n",
      " [0.127 0.122 0.145 0.11 ]]\n",
      "32.26201606879173\n",
      "\n",
      "\n",
      "32.26201606879173 \n",
      " [[0.19708867 0.19621798 0.19621798 0.19572906]\n",
      " [0.25574138 0.25524507 0.25524507 0.25406404]] \n",
      "\n",
      " [[0.127 0.122 0.145 0.11 ]\n",
      " [0.127 0.122 0.145 0.11 ]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MAE + L2\n",
    "reg_coeff = 0.05\n",
    "loss1 = torch.mean(torch.abs(torch.matmul(x_t, w_t) - y_t))\n",
    "loss1.backward()\n",
    "assert(np.allclose(w_t.grad.data.numpy(), LossAndDerivatives.mae_derivative(x_n, y_n, w), rtol=1e-3))\n",
    "\n",
    "print(w_t.grad.data.numpy())\n",
    "w_t.grad.data.zero_()\n",
    "print()\n",
    "\n",
    "loss2 = reg_coeff * torch.sum(w_t**2)\n",
    "loss2.backward()\n",
    "print(w_t.grad.data.numpy())\n",
    "w_t.grad.data.zero_()\n",
    "print((loss1 + loss2).item())\n",
    "print()\n",
    "print()\n",
    "\n",
    "loss_n = np.mean(np.abs((x_n.dot(w) - y_n))) + reg_coeff * np.sum(w**2)\n",
    "print(loss_n, '\\n', LossAndDerivatives.mae_derivative(x_n, y_n, w), '\\n\\n', reg_coeff * LossAndDerivatives.l2_reg_derivative(w), '\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.19708867 0.19621798 0.19621798 0.19572906]\n",
      " [0.25574138 0.25524507 0.25524507 0.25406404]]\n",
      "\n",
      "[[0.05 0.05 0.05 0.05]\n",
      " [0.05 0.05 0.05 0.05]]\n",
      "32.12463606879173\n",
      "\n",
      "\n",
      "32.12463606879173 \n",
      " [[0.19708867 0.19621798 0.19621798 0.19572906]\n",
      " [0.25574138 0.25524507 0.25524507 0.25406404]] \n",
      "\n",
      " [[0.05 0.05 0.05 0.05]\n",
      " [0.05 0.05 0.05 0.05]] \n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# MAE + L1\n",
    "reg_coeff = 0.05\n",
    "loss1 = torch.mean(torch.abs(torch.matmul(x_t, w_t) - y_t))\n",
    "loss1.backward()\n",
    "assert(np.allclose(w_t.grad.data.numpy(), LossAndDerivatives.mae_derivative(x_n, y_n, w), rtol=1e-3))\n",
    "\n",
    "print(w_t.grad.data.numpy())\n",
    "w_t.grad.data.zero_()\n",
    "print()\n",
    "\n",
    "loss2 = reg_coeff * torch.sum(torch.abs(w_t))\n",
    "loss2.backward()\n",
    "print(w_t.grad.data.numpy())\n",
    "w_t.grad.data.zero_()\n",
    "print((loss1 + loss2).item())\n",
    "print()\n",
    "print()\n",
    "\n",
    "loss_n = np.mean(np.abs((x_n.dot(w) - y_n))) + reg_coeff * np.sum(np.abs(w))\n",
    "print(loss_n, '\\n', LossAndDerivatives.mae_derivative(x_n, y_n, w), '\\n\\n', reg_coeff * LossAndDerivatives.l1_reg_derivative(w), '\\n\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here comes small loop with gradient descent algorithm. We compute the gradient over the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_w_by_grad(X, Y, w_0, loss_mode='mse', reg_mode=None, lr=0.05, n_steps=100, reg_coeff=0.05):\n",
    "    if loss_mode == 'mse':\n",
    "        loss_function = LossAndDerivatives.mse\n",
    "        loss_derivative = LossAndDerivatives.mse_derivative\n",
    "    elif loss_mode == 'mae':\n",
    "        loss_function = LossAndDerivatives.mae\n",
    "        loss_derivative = LossAndDerivatives.mae_derivative\n",
    "    else:\n",
    "        raise ValueError('Unknown loss function. Available loss functions: `mse`, `mae`')\n",
    "    \n",
    "    if reg_mode is None:\n",
    "        reg_function = LossAndDerivatives.no_reg\n",
    "        reg_derivative = LossAndDerivatives.no_reg_derivative # lambda w: np.zeros_like(w)\n",
    "    elif reg_mode == 'l2':\n",
    "        reg_function = LossAndDerivatives.l2_reg\n",
    "        reg_derivative = LossAndDerivatives.l2_reg_derivative\n",
    "    elif reg_mode == 'l1':\n",
    "        reg_function = LossAndDerivatives.l1_reg\n",
    "        reg_derivative = LossAndDerivatives.l1_reg_derivative\n",
    "    else:\n",
    "        raise ValueError('Unknown regularization mode. Avaliable modes: `l1`, `l2`, None')\n",
    "    \n",
    "    \n",
    "    w = w_0.copy()\n",
    "\n",
    "    for i in range(n_steps):\n",
    "        empirical_risk = loss_function(X, Y, w) + reg_coeff * reg_function(w)\n",
    "        gradient = loss_derivative(X, Y, w) + reg_coeff * reg_derivative(w)\n",
    "        gradient_norm = np.linalg.norm(gradient)\n",
    "        if gradient_norm > 5.:\n",
    "            gradient = gradient / gradient_norm * 5.\n",
    "        w -= lr * gradient\n",
    "        \n",
    "        if i % 5 == 0:\n",
    "            print('Step={}, loss={}, gradient values={}\\n'.format(i, empirical_risk, gradient))\n",
    "    return w\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial weights matrix\n",
    "w = np.ones((2,2), dtype=float)\n",
    "y_n = np.hstack([targets[:, None], targets[:, None]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step=0, loss=[[231.43353985 231.43353985]\n",
      " [231.43353985 231.43353985]], gradient values=[[2.14961307 2.14961307]\n",
      " [2.8069848  2.8069848 ]]\n",
      "\n",
      "Step=5, loss=[[192.95517061 192.95517061]\n",
      " [192.91338737 192.91338737]], gradient values=[[2.1330249  2.1330249 ]\n",
      " [2.81961075 2.81961075]]\n",
      "\n",
      "Step=10, loss=[[159.06234987 159.06234987]\n",
      " [159.1459271  159.1459271 ]], gradient values=[[2.09499104 2.09499104]\n",
      " [2.84798394 2.84798394]]\n",
      "\n",
      "Step=15, loss=[[129.93842131 129.93842131]\n",
      " [130.0703677  130.0703677 ]], gradient values=[[2.0627539  2.0627539 ]\n",
      " [2.87141888 2.87141888]]\n",
      "\n",
      "Step=20, loss=[[105.34654361 105.34654361]\n",
      " [105.53083265 105.53083265]], gradient values=[[2.0172614  2.0172614 ]\n",
      " [2.90355927 2.90355927]]\n",
      "\n",
      "Step=25, loss=[[85.27441044 85.27441044]\n",
      " [85.51673558 85.51673558]], gradient values=[[1.94816657 1.94816657]\n",
      " [2.95036388 2.95036388]]\n",
      "\n",
      "Step=30, loss=[[69.69950859 69.69950859]\n",
      " [70.00872404 70.00872404]], gradient values=[[1.83071991 1.83071991]\n",
      " [3.02464289 3.02464289]]\n",
      "\n",
      "Step=35, loss=[[58.57321947 58.57321947]\n",
      " [58.96499636 58.96499636]], gradient values=[[1.58880585 1.58880585]\n",
      " [3.15843252 3.15843252]]\n",
      "\n",
      "Step=40, loss=[[51.98579933 51.98579933]\n",
      " [52.48827074 52.48827074]], gradient values=[[0.66399591 0.66399591]\n",
      " [2.44720534 2.44720534]]\n",
      "\n",
      "Step=45, loss=[[49.72905019 49.72905019]\n",
      " [50.33699763 50.33699763]], gradient values=[[-0.10747635 -0.10747635]\n",
      " [ 1.45976783  1.45976783]]\n",
      "\n",
      "Step=50, loss=[[48.78907668 48.78907668]\n",
      " [49.49134365 49.49134365]], gradient values=[[-0.43832385 -0.43832385]\n",
      " [ 0.99440717  0.99440717]]\n",
      "\n",
      "Step=55, loss=[[48.19028504 48.19028504]\n",
      " [48.9795724  48.9795724 ]], gradient values=[[-0.56992072 -0.56992072]\n",
      " [ 0.76705377  0.76705377]]\n",
      "\n",
      "Step=60, loss=[[47.71060293 47.71060293]\n",
      " [48.58147131 48.58147131]], gradient values=[[-0.61188886 -0.61188886]\n",
      " [ 0.64860316  0.64860316]]\n",
      "\n",
      "Step=65, loss=[[47.29321007 47.29321007]\n",
      " [48.24116715 48.24116715]], gradient values=[[-0.61399125 -0.61399125]\n",
      " [ 0.58036049  0.58036049]]\n",
      "\n",
      "Step=70, loss=[[46.9199545  46.9199545 ]\n",
      " [47.94103538 47.94103538]], gradient values=[[-0.59879631 -0.59879631]\n",
      " [ 0.53562298  0.53562298]]\n",
      "\n",
      "Step=75, loss=[[46.58286957 46.58286957]\n",
      " [47.67344146 47.67344146]], gradient values=[[-0.57651872 -0.57651872]\n",
      " [ 0.50222501  0.50222501]]\n",
      "\n",
      "Step=80, loss=[[46.27721296 46.27721296]\n",
      " [47.433882   47.433882  ]], gradient values=[[-0.55176216 -0.55176216]\n",
      " [ 0.47460744  0.47460744]]\n",
      "\n",
      "Step=85, loss=[[45.99952722 45.99952722]\n",
      " [47.21909188 47.21909188]], gradient values=[[-0.52658275 -0.52658275]\n",
      " [ 0.4502143   0.4502143 ]]\n",
      "\n",
      "Step=90, loss=[[45.74700083 45.74700083]\n",
      " [47.02642677 47.02642677]], gradient values=[[-0.50188089 -0.50188089]\n",
      " [ 0.42785556  0.42785556]]\n",
      "\n",
      "Step=95, loss=[[45.51721869 45.51721869]\n",
      " [46.85362345 46.85362345]], gradient values=[[-0.47803347 -0.47803347]\n",
      " [ 0.40696315  0.40696315]]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "w_grad = get_w_by_grad(x_n, y_n, w, lr=0.05, loss_mode='mse', reg_mode='l1', reg_coeff=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.scatter(x_n[:, -1], y_n[:, -1])\n",
    "plt.scatter(x_n[:, -1], x_n.dot(w_grad)[:, -1], color='orange', linewidth=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Submitting your work\n",
    "To submit your work you need to log into Yandex contest (link will be provided later) and upload the `loss_and_derivatives.py` file for the corresponding problem"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3 research env",
   "language": "python",
   "name": "py3_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
