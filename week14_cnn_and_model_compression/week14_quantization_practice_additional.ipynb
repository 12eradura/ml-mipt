{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN Quantization Practice\n",
    "\n",
    "This tutorial implements [DoReFa-Net](https://arxiv.org/abs/1606.06160) weight and activation quantization scheme. Gradient quantization is not implemented, however is described in the original paper.\n",
    "\n",
    "Code inspiration is taken from randomly found [github](https://github.com/zzzxxxttt/pytorch_DoReFaNet) due to the fact that current notebook author has a lack of creativity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = True\n",
    "torch.manual_seed(42)\n",
    "device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "kwargs = {'num_workers': 1, 'pin_memory': True} if use_cuda else {}\n",
    "batch_size = 256\n",
    "test_batch_size = 128\n",
    "epochs = 10\n",
    "lr = 1e-3\n",
    "weight_decay = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's use MNIST dataset and evaluate one of the current state-of-the-art methods of CNN quantization.\n",
    "\n",
    "We will implement a simple CNN model yet capable of reaching high performance on MNIST.\n",
    "\n",
    "Then we will implement a quantized CNN capable of reaching same performance on it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data loading\n",
    "\n",
    "We are developing quantized neural networks, so we need to feed images into CNN in different manner:\n",
    "pixel absolute values are no longer needed to be less than 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_signed_char(tensor):\n",
    "    # note that we convert image values from float32 range([-0.5, 0.5]) to signed char range [-128,127]\n",
    "    return torch.floor(255.0 * tensor)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=True, download=True,\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.5,), (1, )),\n",
    "                       \n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, **kwargs)\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, transform=transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (1, )),\n",
    "                   ])),\n",
    "    batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Training and Validation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, device, train_loader, optimizer, epoch, **kwargs):\n",
    "    log_interval = kwargs.get('log_interval', 10)\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(to_signed_char(data))\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward(retain_graph=True)\n",
    "        optimizer.step()\n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "\n",
    "def test(model, device, test_loader, **kwargs):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(to_signed_char(data))\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()  # sum up batch loss\n",
    "            pred = output.argmax(dim=1, keepdim=True)  # get the index of the max log-probability\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        100. * correct / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CNN Model\n",
    "\n",
    "Let's implement something really easy, __conv->bn->relu->pool__ will be enough."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, 3, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.maxpool_2x2 = nn.MaxPool2d((2,2))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(64, 10)\n",
    "        self.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool_2x2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.gmp(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.674997\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 1.654323\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.973233\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.655535\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.411349\n",
      "\n",
      "Test set: Average loss: 0.3281, Accuracy: 9420/10000 (94%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.294506\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.289305\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.231456\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.225936\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.241886\n",
      "\n",
      "Test set: Average loss: 0.1677, Accuracy: 9603/10000 (96%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.134363\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.119198\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.181269\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.154401\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.138122\n",
      "\n",
      "Test set: Average loss: 0.1234, Accuracy: 9689/10000 (97%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.107432\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.102766\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.135344\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.107568\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.105222\n",
      "\n",
      "Test set: Average loss: 0.1033, Accuracy: 9734/10000 (97%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.067680\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.077396\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.101579\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.112761\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.110032\n",
      "\n",
      "Test set: Average loss: 0.0924, Accuracy: 9750/10000 (98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.094820\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.040319\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.079487\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.094148\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.076127\n",
      "\n",
      "Test set: Average loss: 0.0860, Accuracy: 9754/10000 (98%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.060780\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.070188\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.088920\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.097815\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.075603\n",
      "\n",
      "Test set: Average loss: 0.0790, Accuracy: 9779/10000 (98%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.092589\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.046142\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.082922\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.055269\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.058473\n",
      "\n",
      "Test set: Average loss: 0.0747, Accuracy: 9774/10000 (98%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.094429\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.074735\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.047025\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.060373\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.064304\n",
      "\n",
      "Test set: Average loss: 0.0712, Accuracy: 9790/10000 (98%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.099610\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.037451\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.061265\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.064498\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.060435\n",
      "\n",
      "Test set: Average loss: 0.0705, Accuracy: 9780/10000 (98%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, log_interval=50)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Quantization Layers\n",
    "\n",
    "Not let's implement basic building blocks for DoReFa-Net quantization scheme.\n",
    "Firstly, we will need to implement uniform quantizer.\n",
    "\n",
    "Given real value $r\\in [0;1]$ uniform quantized value $r_0$ may be calculated:\n",
    "\n",
    "$r_0 = \\frac{1}{2^k-1} round((2^k-1) * r)$\n",
    "\n",
    "The calculated value may be saved as floating point number, or converted to fixed point representation by multiplying it on $2^{\\text{n_bits}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uniform_quantizer(n_bits):\n",
    "    class qfn(torch.autograd.Function):\n",
    "        @staticmethod\n",
    "        def forward(ctx, in_tensor):\n",
    "            if n_bits == 32:\n",
    "                # do not apply quantizer for full-bitwidth values\n",
    "                out_tensor = in_tensor\n",
    "            elif n_bits == 1:\n",
    "                # Binary Weight Net implementation\n",
    "                out_tensor = torch.sign(in_tensor)\n",
    "            else:\n",
    "                n = float(2 ** n_bits - 1)\n",
    "                out = torch.round(in_tensor * n) / n\n",
    "            return out\n",
    "\n",
    "        @staticmethod\n",
    "        def backward(ctx, grad_output):\n",
    "            grad_input = grad_output.clone()\n",
    "            return grad_input\n",
    "    return qfn().apply"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we will need separate quantization functions for weights and activations.\n",
    "For weights the following formula is implemented:\n",
    "$$\n",
    "r_0 = 2 \\cdot\\textit{quantize_k}(\\frac{tanh(r)}{2 * max|tanh(r)|} + \\frac{1}{2}) - 1\n",
    "$$\n",
    "For activations uniform quantization is applied, as defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class weight_quantize_fn(nn.Module):\n",
    "    def __init__(self, n_bits):\n",
    "        super(weight_quantize_fn, self).__init__()\n",
    "        assert n_bits <= 8 or n_bits == 32\n",
    "        self.n_bits = n_bits\n",
    "        self.value_range = 2 ** self.n_bits - 1\n",
    "        self.uniform_q = get_uniform_quantizer(n_bits)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.n_bits == 32:\n",
    "            weight_q = x\n",
    "        elif self.n_bits == 1:\n",
    "            E = x.abs().mean().detach()\n",
    "            weight_q = self.uniform_q(x / E) * E\n",
    "        else:\n",
    "            weight = x.tanh()\n",
    "            weight = weight / 2 / weight.abs().max() + 0.5\n",
    "            # weight_q = 2 * self.uniform_q(weight) - 1 # for original implementation with real values\n",
    "            weight_q = self.uniform_q(weight)\n",
    "            weight_q = self.value_range * weight_q - (self.value_range + 1) / 2\n",
    "        return weight_q\n",
    "\n",
    "\n",
    "class activation_quantize_fn(nn.Module):\n",
    "    def __init__(self, n_bits):\n",
    "        super(activation_quantize_fn, self).__init__()\n",
    "        assert n_bits <= 8 or n_bits == 32\n",
    "        self.n_bits = n_bits\n",
    "        self.uniform_q = get_uniform_quantizer(n_bits)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if self.n_bits == 32:\n",
    "            activation_q = x\n",
    "        else:\n",
    "            activation_q = (2 ** self.n_bits - 1) * self.uniform_q(torch.clamp(x, 0, 1))\n",
    "        return activation_q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now our building blocks allow us to implement basic convolution and activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DRFConv2d(nn.Conv2d):\n",
    "    def __init__(self, n_bits, in_channels, out_channels, kernel_size, stride=1,\n",
    "                 padding=0, dilation=1, groups=1, bias=False):\n",
    "        super(DRFConv2d, self).__init__(in_channels, out_channels, kernel_size, stride,\n",
    "                                 padding, dilation, groups, bias)\n",
    "        self.n_bits = n_bits\n",
    "        self.quantize_fn = weight_quantize_fn(n_bits)\n",
    "\n",
    "    def forward(self, in_tensor, order=None):\n",
    "        weight_q = self.quantize_fn(self.weight)\n",
    "        return F.conv2d(in_tensor, weight_q, self.bias, self.stride,\n",
    "                  self.padding, self.dilation, self.groups)\n",
    "\n",
    "class DRFActivation(nn.Module):\n",
    "    def __init__(self, n_bits):\n",
    "        super(DRFActivation, self).__init__()\n",
    "        self.n_bits = n_bits\n",
    "        self.quantize_fn = activation_quantize_fn(n_bits)\n",
    "\n",
    "    def forward(self, in_tensor):\n",
    "        if self.n_bits >= 32:\n",
    "            return in_tensor\n",
    "        out_tensor = self.quantize_fn(in_tensor)\n",
    "        return out_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's implement the quantized CNN the same as the full-precision CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNet(nn.Module):\n",
    "    def __init__(self, n_bits=8):\n",
    "        super(QNet, self).__init__()\n",
    "        self.n_bits = n_bits\n",
    "        self.conv1 = nn.Sequential(\n",
    "            DRFConv2d(self.n_bits, 1, 32, 3, 1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            DRFActivation(self.n_bits)\n",
    "        )\n",
    "        self.maxpool_2x2 = nn.MaxPool2d((2,2))\n",
    "        self.conv2 = nn.Sequential(\n",
    "            DRFConv2d(self.n_bits, 32, 64, 3, 1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            DRFActivation(self.n_bits)\n",
    "        )\n",
    "        self.fc1 = DRFConv2d(self.n_bits, 64, 10, 1)\n",
    "        self.gmp = nn.AdaptiveMaxPool2d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.maxpool_2x2(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.gmp(x)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        output = x / 100000\n",
    "        output = F.log_softmax(output, dim=1)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-4.024532  -4.1768517 -1.2042519 -2.3557918 -4.7003117 -2.1345918\n",
      "  -1.4477618 -2.276712  -2.8576517 -3.0049517]]\n"
     ]
    }
   ],
   "source": [
    "# sanity check\n",
    "qmodel = QNet().to(torch.device('cpu'))\n",
    "in_tensor = torch.floor(torch.from_numpy(np.random.randint(0,255,size=(28,28))).view(1, 1, 28, 28).type(torch.float)) - 127\n",
    "res = qmodel(in_tensor)\n",
    "print(res.detach().numpy()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.814210\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 1.667983\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 1.094900\n",
      "\n",
      "Test set: Average loss: 0.9594, Accuracy: 8672/10000 (87%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.958289\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.761198\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.606827\n",
      "\n",
      "Test set: Average loss: 0.5718, Accuracy: 9322/10000 (93%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.606156\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.508972\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.509803\n",
      "\n",
      "Test set: Average loss: 0.4224, Accuracy: 9476/10000 (95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.438360\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.341214\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.327050\n",
      "\n",
      "Test set: Average loss: 0.3320, Accuracy: 9567/10000 (96%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.344893\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.340173\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.300783\n",
      "\n",
      "Test set: Average loss: 0.2886, Accuracy: 9613/10000 (96%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.304649\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.303052\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.231974\n",
      "\n",
      "Test set: Average loss: 0.2612, Accuracy: 9646/10000 (96%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.291403\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.213703\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.232159\n",
      "\n",
      "Test set: Average loss: 0.2435, Accuracy: 9646/10000 (96%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.203694\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.244823\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.261774\n",
      "\n",
      "Test set: Average loss: 0.2287, Accuracy: 9671/10000 (97%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.256067\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.183208\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.260472\n",
      "\n",
      "Test set: Average loss: 0.2205, Accuracy: 9674/10000 (97%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.244774\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.218757\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.211896\n",
      "\n",
      "Test set: Average loss: 0.2096, Accuracy: 9684/10000 (97%)\n",
      "\n",
      "Train Epoch: 11 [0/60000 (0%)]\tLoss: 0.200575\n",
      "Train Epoch: 11 [25600/60000 (43%)]\tLoss: 0.209064\n",
      "Train Epoch: 11 [51200/60000 (85%)]\tLoss: 0.192527\n",
      "\n",
      "Test set: Average loss: 0.2043, Accuracy: 9683/10000 (97%)\n",
      "\n",
      "Train Epoch: 12 [0/60000 (0%)]\tLoss: 0.195480\n",
      "Train Epoch: 12 [25600/60000 (43%)]\tLoss: 0.195764\n",
      "Train Epoch: 12 [51200/60000 (85%)]\tLoss: 0.203017\n",
      "\n",
      "Test set: Average loss: 0.1998, Accuracy: 9691/10000 (97%)\n",
      "\n",
      "Train Epoch: 13 [0/60000 (0%)]\tLoss: 0.206802\n",
      "Train Epoch: 13 [25600/60000 (43%)]\tLoss: 0.158302\n",
      "Train Epoch: 13 [51200/60000 (85%)]\tLoss: 0.183911\n",
      "\n",
      "Test set: Average loss: 0.1943, Accuracy: 9695/10000 (97%)\n",
      "\n",
      "Train Epoch: 14 [0/60000 (0%)]\tLoss: 0.225192\n",
      "Train Epoch: 14 [25600/60000 (43%)]\tLoss: 0.156648\n",
      "Train Epoch: 14 [51200/60000 (85%)]\tLoss: 0.187886\n",
      "\n",
      "Test set: Average loss: 0.1910, Accuracy: 9703/10000 (97%)\n",
      "\n",
      "Train Epoch: 15 [0/60000 (0%)]\tLoss: 0.165960\n",
      "Train Epoch: 15 [25600/60000 (43%)]\tLoss: 0.171712\n",
      "Train Epoch: 15 [51200/60000 (85%)]\tLoss: 0.148357\n",
      "\n",
      "Test set: Average loss: 0.1853, Accuracy: 9710/10000 (97%)\n",
      "\n",
      "Train Epoch: 16 [0/60000 (0%)]\tLoss: 0.194075\n",
      "Train Epoch: 16 [25600/60000 (43%)]\tLoss: 0.175152\n",
      "Train Epoch: 16 [51200/60000 (85%)]\tLoss: 0.158674\n",
      "\n",
      "Test set: Average loss: 0.1825, Accuracy: 9712/10000 (97%)\n",
      "\n",
      "Train Epoch: 17 [0/60000 (0%)]\tLoss: 0.155286\n",
      "Train Epoch: 17 [25600/60000 (43%)]\tLoss: 0.162643\n",
      "Train Epoch: 17 [51200/60000 (85%)]\tLoss: 0.226259\n",
      "\n",
      "Test set: Average loss: 0.1788, Accuracy: 9713/10000 (97%)\n",
      "\n",
      "Train Epoch: 18 [0/60000 (0%)]\tLoss: 0.199566\n",
      "Train Epoch: 18 [25600/60000 (43%)]\tLoss: 0.169338\n",
      "Train Epoch: 18 [51200/60000 (85%)]\tLoss: 0.158573\n",
      "\n",
      "Test set: Average loss: 0.1779, Accuracy: 9718/10000 (97%)\n",
      "\n",
      "Train Epoch: 19 [0/60000 (0%)]\tLoss: 0.143289\n",
      "Train Epoch: 19 [25600/60000 (43%)]\tLoss: 0.171971\n",
      "Train Epoch: 19 [51200/60000 (85%)]\tLoss: 0.156283\n",
      "\n",
      "Test set: Average loss: 0.1748, Accuracy: 9724/10000 (97%)\n",
      "\n",
      "Train Epoch: 20 [0/60000 (0%)]\tLoss: 0.170464\n",
      "Train Epoch: 20 [25600/60000 (43%)]\tLoss: 0.184536\n",
      "Train Epoch: 20 [51200/60000 (85%)]\tLoss: 0.153351\n",
      "\n",
      "Test set: Average loss: 0.1724, Accuracy: 9732/10000 (97%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = QNet().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=0.9)\n",
    "for epoch in range(1, epochs + 1):\n",
    "    train(model, device, train_loader, optimizer, epoch, log_interval=100)\n",
    "    test(model, device, test_loader)\n",
    "    scheduler.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
